{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diyavol76/DL_training/blob/main/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "78193e36-93ec-4dfb-cc0d-fcbccf4ab3b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-03 14:05:09--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.111.138, 108.177.111.100, 108.177.111.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.111.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/jhu31vt3ph2lusagf6789i6p2uhjlg58/1648994700000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-04-03 14:05:11--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/jhu31vt3ph2lusagf6789i6p2uhjlg58/1648994700000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 142.250.125.132, 2607:f8b0:4001:c2f::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|142.250.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   126MB/s    in 0.5s    \n",
            "\n",
            "2022-04-03 14:05:12 (126 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "77b2622d-1a7b-444a-892c-a2953cfabf65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "1bec134f-3101-4097-f5ff-304a5501fb0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "0f53758f-9fa4-44e5-8252-c1b874e9952c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 8s 13ms/step - loss: 6.0062 - accuracy: 0.0242\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.4398 - accuracy: 0.0399\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.3614 - accuracy: 0.0439\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.3046 - accuracy: 0.0414\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.2357 - accuracy: 0.0388\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.1621 - accuracy: 0.0439\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.0947 - accuracy: 0.0585\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.0318 - accuracy: 0.0575\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.9631 - accuracy: 0.0782\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.8782 - accuracy: 0.0767\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.7930 - accuracy: 0.0797\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.6959 - accuracy: 0.0913\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.6029 - accuracy: 0.1034\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.5166 - accuracy: 0.1120\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.4256 - accuracy: 0.1155\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.3290 - accuracy: 0.1266\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.2448 - accuracy: 0.1428\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.1520 - accuracy: 0.1564\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.0685 - accuracy: 0.1645\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.9733 - accuracy: 0.1837\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.8895 - accuracy: 0.2139\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.8061 - accuracy: 0.2301\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.7297 - accuracy: 0.2482\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.6475 - accuracy: 0.2714\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.5793 - accuracy: 0.2780\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.5065 - accuracy: 0.2982\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.4234 - accuracy: 0.3204\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.3453 - accuracy: 0.3224\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.2885 - accuracy: 0.3416\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.2142 - accuracy: 0.3628\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.1505 - accuracy: 0.3744\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.0825 - accuracy: 0.3875\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 3.0218 - accuracy: 0.4051\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.9626 - accuracy: 0.4152\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.8885 - accuracy: 0.4218\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.8320 - accuracy: 0.4324\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.7847 - accuracy: 0.4369\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.7319 - accuracy: 0.4475\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.6572 - accuracy: 0.4642\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.6028 - accuracy: 0.4783\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.5431 - accuracy: 0.4879\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.5033 - accuracy: 0.4859\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.4575 - accuracy: 0.4985\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.3991 - accuracy: 0.5101\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.3508 - accuracy: 0.5126\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.3157 - accuracy: 0.5288\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.2707 - accuracy: 0.5409\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.2254 - accuracy: 0.5479\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.1873 - accuracy: 0.5469\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.1476 - accuracy: 0.5661\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.0956 - accuracy: 0.5853\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.0660 - accuracy: 0.5767\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 2.0432 - accuracy: 0.5817\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.9992 - accuracy: 0.5883\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.9497 - accuracy: 0.6075\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.9177 - accuracy: 0.6155\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8890 - accuracy: 0.6211\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8521 - accuracy: 0.6211\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.8275 - accuracy: 0.6312\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.7816 - accuracy: 0.6372\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.7410 - accuracy: 0.6509\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.7209 - accuracy: 0.6473\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.6780 - accuracy: 0.6599\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.6413 - accuracy: 0.6615\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.6114 - accuracy: 0.6751\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5954 - accuracy: 0.6776\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.5614 - accuracy: 0.6796\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 1.5303 - accuracy: 0.6953\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 1.5228 - accuracy: 0.6897\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 1.4875 - accuracy: 0.7033\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4565 - accuracy: 0.7003\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4246 - accuracy: 0.7225\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4074 - accuracy: 0.7195\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4061 - accuracy: 0.7170\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.4058 - accuracy: 0.7149\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.3522 - accuracy: 0.7346\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.3064 - accuracy: 0.7447\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2764 - accuracy: 0.7417\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2540 - accuracy: 0.7533\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2319 - accuracy: 0.7619\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2132 - accuracy: 0.7593\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1998 - accuracy: 0.7664\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1759 - accuracy: 0.7689\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1561 - accuracy: 0.7730\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.2280 - accuracy: 0.7543\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1836 - accuracy: 0.7568\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1422 - accuracy: 0.7674\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.1164 - accuracy: 0.7740\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0919 - accuracy: 0.7765\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0732 - accuracy: 0.7906\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0494 - accuracy: 0.7936\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0359 - accuracy: 0.7931\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0229 - accuracy: 0.8012\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9935 - accuracy: 0.8058\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9838 - accuracy: 0.8088\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9841 - accuracy: 0.8027\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.9786 - accuracy: 0.7992\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9734 - accuracy: 0.8063\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9445 - accuracy: 0.8133\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9277 - accuracy: 0.8068\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9180 - accuracy: 0.8163\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8974 - accuracy: 0.8184\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8824 - accuracy: 0.8148\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8956 - accuracy: 0.8108\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.9091 - accuracy: 0.8103\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8732 - accuracy: 0.8224\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8615 - accuracy: 0.8209\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8761 - accuracy: 0.8204\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8893 - accuracy: 0.8133\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8393 - accuracy: 0.8169\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8244 - accuracy: 0.8264\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.8089 - accuracy: 0.8305\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7832 - accuracy: 0.8335\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7687 - accuracy: 0.8385\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7578 - accuracy: 0.8426\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7479 - accuracy: 0.8411\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7367 - accuracy: 0.8456\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7416 - accuracy: 0.8431\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7263 - accuracy: 0.8451\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7164 - accuracy: 0.8476\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.7065 - accuracy: 0.8496\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6984 - accuracy: 0.8496\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6923 - accuracy: 0.8527\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6826 - accuracy: 0.8496\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6814 - accuracy: 0.8517\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6811 - accuracy: 0.8557\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6642 - accuracy: 0.8542\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6535 - accuracy: 0.8628\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6446 - accuracy: 0.8628\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6375 - accuracy: 0.8607\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6339 - accuracy: 0.8602\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6699 - accuracy: 0.8496\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6615 - accuracy: 0.8522\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6464 - accuracy: 0.8562\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6282 - accuracy: 0.8577\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6130 - accuracy: 0.8628\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6047 - accuracy: 0.8607\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.6004 - accuracy: 0.8633\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5870 - accuracy: 0.8678\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5842 - accuracy: 0.8638\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5746 - accuracy: 0.8693\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5607 - accuracy: 0.8734\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5618 - accuracy: 0.8729\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5579 - accuracy: 0.8703\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5518 - accuracy: 0.8739\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5468 - accuracy: 0.8718\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5344 - accuracy: 0.8734\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.5285 - accuracy: 0.8764\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5221 - accuracy: 0.8784\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5159 - accuracy: 0.8784\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5110 - accuracy: 0.8804\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5052 - accuracy: 0.8764\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5027 - accuracy: 0.8814\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4977 - accuracy: 0.8794\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4944 - accuracy: 0.8814\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4910 - accuracy: 0.8845\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4872 - accuracy: 0.8829\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4806 - accuracy: 0.8829\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4750 - accuracy: 0.8819\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4786 - accuracy: 0.8840\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4765 - accuracy: 0.8845\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5077 - accuracy: 0.8754\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5636 - accuracy: 0.8602\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5357 - accuracy: 0.8688\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.5024 - accuracy: 0.8764\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4984 - accuracy: 0.8784\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4822 - accuracy: 0.8819\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4843 - accuracy: 0.8809\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4731 - accuracy: 0.8814\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4606 - accuracy: 0.8850\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4500 - accuracy: 0.8870\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4435 - accuracy: 0.8875\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4388 - accuracy: 0.8910\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4340 - accuracy: 0.8895\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4302 - accuracy: 0.8885\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4229 - accuracy: 0.8895\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4195 - accuracy: 0.8910\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4163 - accuracy: 0.8905\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4223 - accuracy: 0.8930\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4145 - accuracy: 0.8900\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4236 - accuracy: 0.8900\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4108 - accuracy: 0.8940\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4068 - accuracy: 0.8890\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.4016 - accuracy: 0.8946\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3952 - accuracy: 0.8946\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.3881 - accuracy: 0.8940\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.3868 - accuracy: 0.8986\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.3838 - accuracy: 0.8951\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3849 - accuracy: 0.8915\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3768 - accuracy: 0.8976\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3925 - accuracy: 0.8920\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3786 - accuracy: 0.8946\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.3806 - accuracy: 0.8930\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3734 - accuracy: 0.8951\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.3680 - accuracy: 0.8991\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3631 - accuracy: 0.8961\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3598 - accuracy: 0.8991\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3569 - accuracy: 0.8981\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3615 - accuracy: 0.8946\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.3645 - accuracy: 0.8961\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "6689ce44-f0af-48f4-d6c3-9688c2036ef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "194d3b1e-59f6-475c-c6b1-d8d3783fa036",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im feeling chills me sound and tired of every theyre all singing its true what of us would better fine velvet more face more like my kind yourself more my life take good care sender new youll learn see more like deck new new always more up lost from now yourself a break break break break break break break break break care love you and nights things will a walk in the out an none yourself sound feel feel more feel guys smile more patch more our secrets hour fine see out see fine fine fine fine fine fine out go us another\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}